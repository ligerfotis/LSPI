{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class LSPI:\n",
    "    def __init__(self, basis_function, discount, state_size=4):\n",
    "        self.basis_function = basis_function\n",
    "        self.discount = discount\n",
    "        # check if the basis function is linear or not\n",
    "        if isinstance(basis_function, LinearBasisFunction):\n",
    "            self.w = np.zeros(basis_function.size)  # Initialize the weight vector\n",
    "        else:\n",
    "            self.w = np.zeros(basis_function.size * state_size+1)  # Initialize the weight vector\n",
    "\n",
    "    def Qvalue(self, state, action):\n",
    "        phi = self.basis_function.evaluate(state, action)\n",
    "        return np.dot(self.w, phi)\n",
    "\n",
    "    def policy(self, state):\n",
    "        # In the CartPole environment, there are only two possible actions: 0 and 1\n",
    "        q_values = [self.Qvalue(state, a) for a in [0, 1]]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # This is a simplified version of the update rule in the provided code\n",
    "        phi = self.basis_function.evaluate(state, action)\n",
    "        if next_state is not None:\n",
    "            q_next = max([self.Qvalue(next_state, a) for a in [0, 1]])\n",
    "        else:\n",
    "            q_next = 0\n",
    "        target = reward + self.discount * q_next\n",
    "        error = target - self.Qvalue(state, action)\n",
    "        self.w += 0.01 * error * phi  # Assume a constant learning rate of 0.01\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class LinearBasisFunction:\n",
    "    def __init__(self):\n",
    "        self.size = 5  # 4 dimensions for the state and 1 for the action\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        return np.concatenate([state, [action]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class RadialBasisFunction:\n",
    "    def __init__(self, num_centers):\n",
    "        self.size = num_centers\n",
    "        self.centers = np.linspace(-1, 1, num_centers)\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        return np.concatenate([np.exp(-np.square(state - c)) for c in self.centers] + [np.array([action])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  0\n",
      "Episode: 0, reward: 8.0\n",
      "count:  0\n",
      "Episode: 10000, reward: 10.0\n",
      "count:  0\n",
      "Episode: 20000, reward: 9.0\n",
      "count:  0\n",
      "Episode: 30000, reward: 9.0\n",
      "count:  0\n",
      "Episode: 40000, reward: 8.0\n",
      "count:  0\n",
      "Episode: 50000, reward: 10.0\n",
      "count:  0\n",
      "Episode: 60000, reward: 9.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m reward_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# Select an action\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mlspi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Execute the action\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     next_state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "Cell \u001B[0;32mIn[39], line 17\u001B[0m, in \u001B[0;36mLSPI.policy\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpolicy\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# In the CartPole environment, there are only two possible actions: 0 and 1\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQvalue(state, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax(q_values)\n",
      "Cell \u001B[0;32mIn[39], line 17\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpolicy\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# In the CartPole environment, there are only two possible actions: 0 and 1\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax(q_values)\n",
      "Cell \u001B[0;32mIn[39], line 12\u001B[0m, in \u001B[0;36mLSPI.Qvalue\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mQvalue\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[0;32m---> 12\u001B[0m     phi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbasis_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw, phi)\n",
      "Cell \u001B[0;32mIn[41], line 7\u001B[0m, in \u001B[0;36mRadialBasisFunction.evaluate\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mconcatenate([np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39msquare(state \u001B[38;5;241m-\u001B[39m c)) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcenters] \u001B[38;5;241m+\u001B[39m [np\u001B[38;5;241m.\u001B[39marray([action])])\n",
      "Cell \u001B[0;32mIn[41], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mconcatenate([np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msquare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcenters] \u001B[38;5;241m+\u001B[39m [np\u001B[38;5;241m.\u001B[39marray([action])])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# Create the basis function\n",
    "basis_function = LinearBasisFunction()\n",
    "basis_function = RadialBasisFunction(4)\n",
    "\n",
    "# Initialize LSPI with the basis function and a discount factor\n",
    "lspi = LSPI(basis_function, discount=0.99)\n",
    "\n",
    "# Main training loop\n",
    "num_episodes = 100000\n",
    "for i_episode in range(num_episodes):\n",
    "    # Reset the environment and state\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    while not done:\n",
    "        # Select an action\n",
    "        action = lspi.policy(state)\n",
    "\n",
    "        # Execute the action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = None if done else next_state\n",
    "\n",
    "        # Update the policy\n",
    "        lspi.update(state, action, reward, next_state)\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    if i_episode % 10000 == 0:\n",
    "        # render a game\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        count = 0\n",
    "        while not done:\n",
    "            if count > 500:\n",
    "                break\n",
    "            env.render()\n",
    "            action = lspi.policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = None if done else next_state\n",
    "            state = next_state\n",
    "        print(\"count: \", count)\n",
    "        print(\"Episode: {}, reward: {}\".format(i_episode, reward_sum))\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
